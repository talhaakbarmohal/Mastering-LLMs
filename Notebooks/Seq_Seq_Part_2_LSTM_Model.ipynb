{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnvv1XXrtnbkt+4gkXg8vW"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **The Need for LSTM: Addressing RNN Limitations**\n",
        "While **Recurrent Neural Networks (RNNs) **are designed for sequential data, they often excel primarily at short-term memory. This limitation arises due to two main challenges:\n",
        "\n",
        "**Long-Term Dependencies:** RNNs struggle to remember relationships between elements separated by long distances in a sequence. For example, understanding a sentence's meaning might require connecting its first and last words, which traditional RNNs often fail to achieve.\n",
        "\n",
        "**Vanishing Gradients:** During training, the gradients used to update the network's weights can become extremely small for earlier layers. This \"vanishing gradient\" problem prevents earlier layers from learning effectively, hindering the RNN's ability to capture long-term dependencies.\n",
        "\n",
        "These shortcomings motivated the development of Long Short-Term Memory (LSTM) networks. LSTMs are specifically designed to address these issues and effectively model long-range dependencies in sequential data, making them a powerful tool for various text processing tasks."
      ],
      "metadata": {
        "id": "Db20W1GPrbT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How LSTMs Handle RNN Limitations\n",
        "LSTMs tackle the challenges of traditional RNNs through their unique architecture:\n",
        "\n",
        "**Memory Cells:** LSTMs introduce memory cells that act like information storage units. These cells can retain crucial information over long sequences, enabling the network to \"remember\" important context from earlier inputs.\n",
        "\n",
        "**Gating Mechanisms:** LSTMs utilize gates to regulate the flow of information into and out of the memory cells. These gates control what information gets stored, discarded, and utilized, allowing the network to selectively retain and access relevant information over extended periods.\n",
        "\n",
        "By incorporating memory cells and gating mechanisms, LSTMs effectively address the following:\n",
        "\n",
        "Long-Term Dependencies: The ability to store and access information over long sequences enables LSTMs to capture relationships between elements separated by significant distances, mitigating the issue of long-term dependencies.\n",
        "\n",
        "Vanishing Gradients: The gating mechanisms help regulate the flow of gradients during training, preventing them from diminishing too rapidly and allowing earlier layers to learn effectively. This addresses the vanishing gradient problem and enables LSTMs to capture long-term dependencies effectively."
      ],
      "metadata": {
        "id": "FoKLxQIWtiUY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sZAVbGpfjr7"
      },
      "outputs": [],
      "source": [
        "!pip install torch==2.2.2\n",
        "!pip install torchtext==0.17.2\n",
        "!pip install numpy==1.26.0\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# You can also use this section to suppress warnings generated by your code:\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "Iln-3XQSraYc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM: A Deep Dive into Long-Term Memory\n",
        "In RNNs, we typically maintain a single hidden state to capture information from previous inputs. However, LSTMs introduce an additional state called the cell state, which acts as a long-term memory mechanism. This cell state is crucial for retaining crucial information over extended sequences.\n",
        "\n",
        "Along with the cell state, LSTMs incorporate three essential gates that regulate the flow of information into and out of the cell state:\n",
        "\n",
        "**Forget Gate:** This gate decides what information to discard from the cell state. It analyzes the previous hidden state and the current input, assigning a value between 0 and 1 to each element in the cell state. A value of 0 indicates complete forgetting, while 1 signifies retention.\n",
        "\n",
        "**Input Gate:** This gate determines what new information to store in the cell state. It considers the previous hidden state and the current input, selectively updating the cell state with relevant information.\n",
        "\n",
        "**Output Gate:** This gate controls what information from the cell state is outputted to the hidden state. It analyzes the previous hidden state, the current input, and the updated cell state, selectively outputting relevant information to the hidden state.\n",
        "\n",
        "These three gates work together to regulate the flow of information through the LSTM cell, enabling it to capture long-term dependencies and mitigate the vanishing gradient problem. We will discuss each gate in detail in the following sections.\n",
        "\n",
        "I hope this provides a clear introduction to the core components of LSTM networks, setting the stage for a deeper exploration of each gate's functionality.\n"
      ],
      "metadata": {
        "id": "nrvIT_d_xXZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This code is for short term memory, similiar to RNN(Check my RNN model notebook for detail)\n",
        "X = [1.0, 5.0, 7.0, 4.0]  # Input values, processed sequentially\n",
        "W_xh = torch.tensor([-10.0], requires_grad=True)  # Weight applied to the current input (input-to-hidden)\n",
        "W_hh = torch.tensor([10.0], requires_grad=True)  # Weight applied to the previous hidden state (hidden-to-hidden)\n",
        "b_h = torch.tensor([0.0], requires_grad=True)  # Bias term for the hidden state calculation\n",
        "x_t = 1  # Current input value (placeholder)\n",
        "h_prev = torch.tensor([-1.0], requires_grad=True)  # Initial hidden state (responsible for short term memory)\n",
        "W_hy = torch.tensor([4.0], requires_grad=True)  # Weight applied to the hidden state for output (hidden-to-output)\n",
        "b_y = torch.tensor([5.0], requires_grad=True)  # Bias term for the output calculation\n",
        "y_hat_t = torch.tensor([15.0], requires_grad=True)  # Target output (expected value)\n",
        "ct_prev = torch.tensor([0.0], requires_grad=True) # Initial cell state (responsible for long term memory)\n"
      ],
      "metadata": {
        "id": "74c7ayqJuzWN"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forget Gate\n",
        "When we multiply the\n",
        "ùëì\n",
        "ùë°\n",
        "f\n",
        "t\n",
        "‚Äã\n",
        "  vector with\n",
        "ùëê\n",
        "ùë°\n",
        "‚àí\n",
        "1\n",
        "c\n",
        "t‚àí1\n",
        "‚Äã\n",
        " , this vector is responsible for removing the unimportant information from the long-term memory\n",
        "ùëê\n",
        "ùë°\n",
        "c\n",
        "t\n",
        "‚Äã\n",
        " .\n",
        "\n"
      ],
      "metadata": {
        "id": "GVEj-qPPqRJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f_t = torch.sigmoid(W_xh * x_t + W_hh * h_prev) #forget gate activation function\n",
        "W_hf = torch.tensor([0.5], requires_grad=True) #forget gate weight\n",
        "b_f = torch.tensor([0.5], requires_grad=True) #forget gate bias\n",
        "\n",
        "def forget_gate(W_xh, W_hf, b_f, x_t, h_prev):\n",
        "  f_t = torch.sigmoid(torch.matmul(x_t, W_xh) + torch.matmul(h_prev, W_hf) + b_f)\n",
        "  return f_t"
      ],
      "metadata": {
        "id": "Rj2DeJgJuzSC"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Input Gate\n",
        "When we multiply the\n",
        "ùëñ\n",
        "ùë°\n",
        "i\n",
        "t\n",
        "‚Äã\n",
        "  vector with\n",
        "ùëê\n",
        "~\n",
        "ùë°\n",
        "c\n",
        "~\n",
        "  \n",
        "t\n",
        "‚Äã\n",
        " , this vector is responsible for deciding what new information should be added to the long-term memory\n",
        "ùëê\n",
        "ùë°\n",
        "c\n",
        "t\n",
        "‚Äã\n",
        " ."
      ],
      "metadata": {
        "id": "jzbYbIOcqUa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W_xc = torch.tensor([0.5], requires_grad=True) #input gate weight cabduduate cell state\n",
        "W_hc = torch.tensor([0.5], requires_grad=True) #input gate weight hidden state\n",
        "b_c = torch.tensor([0.5], requires_grad=True) #input gate bias candidiate cell state\n",
        "W_xi = torch.tensor([0.5], requires_grad=True) #input gate weight input gate\n",
        "W_hi = torch.tensor([0.5], requires_grad=True) #input gate weight hidden state\n",
        "b_i = torch.tensor([0.5], requires_grad=True) #input gate bias input gate\n",
        "\n",
        "def calculate_candidate_cell_state(W_xc, W_hc, b_c, x_t, h_prev):\n",
        "  c_tilde = torch.tanh(torch.matmul(x_t, W_xc) + torch.matmul(h_prev, W_hc) + b_c)\n",
        "  return c_tilde\n",
        "\n",
        "def input_gate(W_xi, W_hi, b_i, x_t, h_prev):\n",
        "  i_t = torch.sigmoid(torch.matmul(x_t, W_xi) + torch.matmul(h_prev, W_hi) + b_i)\n",
        "  return i_t * calculate_candidate_cell_state(W_xc, W_hc, b_c, x_t, h_prev)"
      ],
      "metadata": {
        "id": "Ukp3jy0DqTwZ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Output Gate\n",
        "When we multiply the\n",
        "ùëú\n",
        "ùë°\n",
        "o\n",
        "t\n",
        "‚Äã\n",
        "  vector with the\n",
        "ùëê\n",
        "ùë°\n",
        "c\n",
        "t\n",
        "‚Äã\n",
        " , this vector is responsible for determining which part of the long-term memory\n",
        "ùëê\n",
        "ùë°\n",
        "c\n",
        "t\n",
        "‚Äã\n",
        "  should be outputted at the current timestep."
      ],
      "metadata": {
        "id": "wQv-dcf6y7lv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W_ho = torch.tensor([0.5], requires_grad=True) #output gate weight hidden state\n",
        "W_oh = torch.tensor([0.5], requires_grad=True) #output gate weight\n",
        "b_o = torch.tensor([0.5], requires_grad=True) #output gate bias\n",
        "\n",
        "def output_gate(W_ho, W_hh, b_o, x_t, h_prev):\n",
        "  o_t = torch.sigmoid(torch.matmul(x_t, W_ho) + torch.matmul(h_prev, W_hh) + b_o)\n",
        "  return o_t * torch.tanh(ct_prev)"
      ],
      "metadata": {
        "id": "g2qY_9Cxy7QW"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x in X:\n",
        "  x_t = torch.tensor([float(x)], requires_grad=True)\n",
        "\n",
        "  ct_prev = ct_prev * forget_gate(W_xh, W_hf, b_f, x_t, h_prev)\n",
        "  ct_prev = ct_prev + input_gate(W_xi, W_hi, b_i, x_t, h_prev)\n",
        "  h_t = output_gate(W_ho, W_hh, b_o, x_t, h_prev)\n",
        "  h_prev=h_t\n",
        "\n",
        "print(h_t)\n",
        "y_t = torch.sigmoid(W_hy * h_t + b_y)\n",
        "print(y_t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaC0d-_FoVAc",
        "outputId": "a966234d-2892-4110-b68e-9b7f473f1f70"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.7355], grad_fn=<MulBackward0>)\n",
            "tensor([0.9996], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RYSOak8viCbg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m37QuIuOuzN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9adkqYhhuzH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V35ebOajuy8A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}